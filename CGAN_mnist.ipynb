{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在mnist数据集下的条件GAN\n",
    "- 条件GAN\n",
    "\n",
    "- 实现\n",
    "\n",
    "本例使用mnist数据集，在GAN的基础上将标签y进行one-hot编码，之后和数据样本进行**连接**作为输入，判别器输入为真实样本，每个样本后面加上对应的标签y的one-hot编码，生成器的输入为随机采样样本，每个样本后附加标签y的one-hot编码，以这种方式实现上述的条件概率分布。\n",
    "\n",
    "- 改进\n",
    "\n",
    "CGAN_mnist-02.ipynp 将y和原始输入先映射成高维向量A, B，在将A,B进行连接，以此实现条件GAN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.realpath('.')\n",
    "print(project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "no_cuda = False\n",
    "cuda_available = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 100\n",
    "SEED = 8\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda_available else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./MNIST_data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./MNIST_data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10 #class number\n",
    "g_input_size = 100 + n_classes    # Random noise dimension coming into generator, per output vector\n",
    "g_output_size = 784   # size of generated output vector\n",
    "\n",
    "d_input_size = 784 + n_classes  # Minibatch size - cardinality of distributions\n",
    "\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
    "\n",
    "\n",
    "d_learning_rate = 2e-4  # 2e-4\n",
    "g_learning_rate = 2e-4\n",
    "optim_betas = (0.9, 0.999)\n",
    "\n",
    "print_interval = 200\n",
    "\n",
    "d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "g_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(g_input_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(1024, g_output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), g_input_size)\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(d_input_size, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), d_input_size)\n",
    "        out = self.model(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D  = Discriminator().to(device)\n",
    "G = Generator().to(device)\n",
    "print(D)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "lr = 0.0002\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lb.fit(list(range(0,n_classes)))\n",
    "   #将标签进行one-hot编码\n",
    "def to_categrical(y: torch.FloatTensor):\n",
    "    y_n = y.numpy()\n",
    "    \n",
    "    y_one_hot = lb.transform(y_n)\n",
    "    floatTensor = torch.FloatTensor(y_one_hot)\n",
    "    return floatTensor\n",
    "\n",
    "#样本和one-hot标签进行连接，以此作为条件生成\n",
    "def concanate_data_label(x, y):\n",
    "#     print(f'dimenion y: {y.shape} {x.shape}')\n",
    "    y_one_hot = to_categrical(y)\n",
    "#     print(y_one_hot.shape)\n",
    "    con = torch.cat((x, y_one_hot), 1)\n",
    "#     print(con.shape)\n",
    "    \n",
    "    return con.to(device)\n",
    "\n",
    "\n",
    "D_losses = []\n",
    "G_losses = []    \n",
    "def train(epoch):\n",
    "    D.train()\n",
    "    G.train()\n",
    "    \n",
    "    D_losses.clear()\n",
    "    G_losses.clear()\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        D.zero_grad()\n",
    "#         print(f'+++++{data.shape}')\n",
    "        data_flatten = data.view(data.size(0), d_input_size-n_classes)\n",
    "#         print(f'----flatten data shape: {data_flatten.shape}')\n",
    "        real_data = concanate_data_label(data_flatten, label)\n",
    "        \n",
    "        for d_index in range(d_steps):\n",
    "            # 1. Train D on real+fake\n",
    "            #  1A: Train D on real\n",
    "            d_real_decision = D(Variable(real_data))\n",
    "#             if d_real_decision.shape[0] != BATCH_SIZE:\n",
    "#                 print(d_real_decision.shape)\n",
    "            d_real_error = criterion(d_real_decision, Variable(torch.ones(data.shape[0],1)).to(device))  # ones = true\n",
    "            d_real_error.backward() # compute/store gradients, but don't change params\n",
    "            \n",
    "            #  1B: Train D on fake note: labels are used as a constration\n",
    "            fake_sample = concanate_data_label(torch.randn(data.shape[0], g_input_size-n_classes), label)\n",
    "#             print(f'fake sample shape: {fake_sample.shape}')\n",
    "            d_gen_input = Variable(fake_sample)\n",
    "                \n",
    "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "            \n",
    "            d_fake_data_con = concanate_data_label(d_fake_data, label)\n",
    "            d_fake_decision = D(d_fake_data_con)\n",
    "            d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(data.shape[0], 1)).to(device))  # zeros = fake\n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "            \n",
    "            d_error = d_real_error + d_fake_error\n",
    "            D_losses.append(d_error.data)\n",
    "\n",
    "        for g_index in range(g_steps):\n",
    "        # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.zero_grad()\n",
    "            #sample label as before\n",
    "            fake_sample = concanate_data_label(torch.randn(data.shape[0], g_input_size-n_classes), label)\n",
    "            gen_input = Variable(fake_sample)\n",
    "            g_fake_data = G(gen_input)\n",
    "            g_fake_data_con = concanate_data_label(g_fake_data, label)\n",
    "            \n",
    "            dg_fake_decision = D(g_fake_data_con)\n",
    "            g_error = criterion(dg_fake_decision, Variable(torch.ones(data.shape[0], 1)).to(device))  # we want to fool, so pretend it's all genuine\n",
    "\n",
    "            g_error.backward()\n",
    "            g_optimizer.step()  # Only optimizes G's parameters\n",
    "            \n",
    "            G_losses.append(g_error.data)\n",
    "        \n",
    "            \n",
    "    print('[%d/%d]: D(x): %.3f, D(G(z)): %.3f' % (\n",
    "        epoch , EPOCH,np.mean(D_losses), np.mean(G_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    G.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_sample():\n",
    "    '''10x10 matrix,row 0-9 represent label 0-9'''\n",
    "    sample = torch.randn(100, g_input_size) #100x110\n",
    "    label = torch.arange(n_classes)\n",
    "\n",
    "    for i in range(100):\n",
    "        s = torch.FloatTensor([i % n_classes]) #s: 0-9\n",
    "        s_c = to_categrical(s)\n",
    "        sample[i, sample.shape[1]-10:] = s_c\n",
    "    \n",
    "    return sample.to(device)\n",
    "\n",
    "\n",
    "# print(fixed_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_path = os.path.join(project_root, 'cgan_results/')\n",
    "if not os.path.exists(des_path):\n",
    "    os.makedirs(des_path, exist_ok=True)\n",
    "\n",
    "import math,  itertools\n",
    "from IPython import display\n",
    "\n",
    "size_figure_grid = 10\n",
    "fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(8, 8)) #10 rows 10 columns\n",
    "for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "    ax[i,j].get_xaxis().set_visible(False)\n",
    "    ax[i,j].get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    display.clear_output(wait=True)\n",
    "    train(epoch)\n",
    "#     test(epoch)\n",
    "    with torch.no_grad():\n",
    "        fixed_sample = get_fixed_sample()\n",
    "        sample = G(fixed_sample).cpu()\n",
    "       \n",
    "        \n",
    "        save_image(sample.view(100, 1, 28, 28),\n",
    "                   f'{des_path}epoch_{epoch}.png', nrow=10)\n",
    "        \n",
    "        for k in range(100):\n",
    "            i = k//10\n",
    "            j = k%10\n",
    "            ax[i,j].cla()\n",
    "            ax[i,j].imshow(sample[k,:].data.cpu().numpy().reshape(28, 28),cmap='Greys')\n",
    "        \n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_old = G(fixed_sample).cpu()\n",
    "# print(sample_old.shape)\n",
    "# sample_new = sample_old[:, 0:sample_old.shape[1]-10]\n",
    "# print(sample_new.shape)\n",
    "# v = sample_new.view(sample.size(0), 1, 28, 28)\n",
    "# v.shape\n",
    "# ax[1,1].imshow(sample_new[1,:].data.cpu().numpy().reshape(28, 28),cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(1, 38):\n",
    "    one_image = f'{des_path}epoch_{i}.png'\n",
    "    images.append(imageio.imread(one_image))\n",
    "imageio.mimsave(f'{des_path}cgan.gif', images, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
