{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = '/home/ubuntu/disk1/pytorch-GAN-CGAN/'\n",
    "os.chdir(project_root)\n",
    "\n",
    "no_cuda = False\n",
    "cuda_available = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 5000\n",
    "SEED = 8\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda_available else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./MNIST_data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./MNIST_data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "g_input_size = 100     # Random noise dimension coming into generator, per output vector\n",
    "g_hidden_size = 256   # Generator complexity\n",
    "g_output_size = 784    # size of generated output vector\n",
    "\n",
    "d_input_size = 784   # Minibatch size - cardinality of distributions\n",
    "d_hidden_size = 256   # Discriminator complexity\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
    "\n",
    "\n",
    "d_learning_rate = 2e-4  # 2e-4\n",
    "g_learning_rate = 2e-4\n",
    "optim_betas = (0.9, 0.999)\n",
    "\n",
    "print_interval = 200\n",
    "\n",
    "d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "g_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(g_input_size, g_hidden_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(g_hidden_size, 512),\n",
    "            nn.Linear(512, g_output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), g_input_size)\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(d_input_size, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, d_hidden_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(d_hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x.view(x.size(0), d_input_size))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Dropout(p=0.3)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Dropout(p=0.3)\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): Linear(in_features=512, out_features=784, bias=True)\n",
      "    (4): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "D  = Discriminator().to(device)\n",
    "G = Generator().to(device)\n",
    "print(D)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "lr = 0.0002\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_idx, (data, label) =enumerate(train_loader).__next__()\n",
    "print(data.shape, label.shape)\n",
    "real_data = data.to(device)\n",
    "d_real_decision = D(Variable(real_data))\n",
    "print(d_real_decision.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_losses = []\n",
    "G_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    D.train()\n",
    "    G.train()\n",
    "    \n",
    "    D_losses.clear()\n",
    "    G_losses.clear()\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        real_data = data.to(device) \n",
    "        D.zero_grad()\n",
    "        \n",
    "        for d_index in range(d_steps):\n",
    "            # 1. Train D on real+fake\n",
    "            #  1A: Train D on real\n",
    "            \n",
    "            d_real_decision = D(Variable(real_data))\n",
    "#             print(d_real_decision.shape, flush=True, end='')\n",
    "#             if d_real_decision.shape[0] != BATCH_SIZE:\n",
    "#                 print(d_real_decision.shape)\n",
    "            d_real_error = criterion(d_real_decision, Variable(torch.ones(d_real_decision.shape[0],1)))  # ones = true\n",
    "            d_real_error.backward() # compute/store gradients, but don't change params\n",
    "\n",
    "            #  1B: Train D on fake\n",
    "            d_gen_input = Variable(torch.rand(BATCH_SIZE, g_input_size))\n",
    "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "            \n",
    "            d_fake_decision = D(d_fake_data)\n",
    "            d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(BATCH_SIZE, 1)))  # zeros = fake\n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "            d_error = d_real_error + d_fake_error\n",
    "            D_losses.append(d_error.detach().numpy())\n",
    "#             print(f'd:{d_error}', end=' ')\n",
    "        for g_index in range(g_steps):\n",
    "        # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.zero_grad()\n",
    "\n",
    "            gen_input = Variable(torch.rand(BATCH_SIZE, g_input_size))\n",
    "            g_fake_data = G(gen_input)\n",
    "            dg_fake_decision = D(g_fake_data)\n",
    "            g_error = criterion(dg_fake_decision, Variable(torch.ones(BATCH_SIZE, 1)))  # we want to fool, so pretend it's all genuine\n",
    "\n",
    "            g_error.backward()\n",
    "            g_optimizer.step()  # Only optimizes G's parameters\n",
    "            \n",
    "            G_losses.append(g_error.detach().numpy())\n",
    "#             print(f'g:{g_error}')\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "        epoch , EPOCH, np.mean(D_losses), np.mean(G_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    G.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5000]: loss_d: 1.258, loss_g: 1.327\n",
      "[2/5000]: loss_d: 1.338, loss_g: 0.975\n",
      "[3/5000]: loss_d: 1.381, loss_g: 0.923\n",
      "[4/5000]: loss_d: 1.394, loss_g: 0.813\n",
      "[5/5000]: loss_d: 1.385, loss_g: 0.812\n",
      "[6/5000]: loss_d: 1.402, loss_g: 0.842\n",
      "[7/5000]: loss_d: 1.396, loss_g: 0.793\n",
      "[8/5000]: loss_d: 1.403, loss_g: 0.765\n",
      "[9/5000]: loss_d: 1.389, loss_g: 0.802\n",
      "[10/5000]: loss_d: 1.396, loss_g: 0.795\n",
      "[11/5000]: loss_d: 1.386, loss_g: 0.913\n",
      "[12/5000]: loss_d: 1.379, loss_g: 0.849\n",
      "[13/5000]: loss_d: 1.388, loss_g: 0.748\n",
      "[14/5000]: loss_d: 1.402, loss_g: 0.834\n",
      "[15/5000]: loss_d: 1.393, loss_g: 0.779\n",
      "[16/5000]: loss_d: 1.402, loss_g: 0.788\n",
      "[17/5000]: loss_d: 1.403, loss_g: 0.773\n",
      "[18/5000]: loss_d: 1.397, loss_g: 0.786\n",
      "[19/5000]: loss_d: 1.390, loss_g: 0.770\n",
      "[20/5000]: loss_d: 1.394, loss_g: 0.776\n",
      "[21/5000]: loss_d: 1.399, loss_g: 0.867\n",
      "[22/5000]: loss_d: 1.399, loss_g: 0.794\n",
      "[23/5000]: loss_d: 1.389, loss_g: 0.774\n",
      "[24/5000]: loss_d: 1.386, loss_g: 0.819\n",
      "[25/5000]: loss_d: 1.375, loss_g: 0.768\n",
      "[26/5000]: loss_d: 1.400, loss_g: 0.782\n",
      "[27/5000]: loss_d: 1.395, loss_g: 0.767\n",
      "[28/5000]: loss_d: 1.391, loss_g: 0.772\n",
      "[29/5000]: loss_d: 1.392, loss_g: 0.806\n",
      "[30/5000]: loss_d: 1.386, loss_g: 0.781\n",
      "[31/5000]: loss_d: 1.388, loss_g: 0.777\n",
      "[32/5000]: loss_d: 1.390, loss_g: 0.792\n",
      "[33/5000]: loss_d: 1.409, loss_g: 0.784\n",
      "[34/5000]: loss_d: 1.382, loss_g: 0.819\n",
      "[35/5000]: loss_d: 1.384, loss_g: 0.786\n",
      "[36/5000]: loss_d: 1.385, loss_g: 0.794\n",
      "[37/5000]: loss_d: 1.401, loss_g: 0.769\n",
      "[38/5000]: loss_d: 1.397, loss_g: 0.781\n",
      "[39/5000]: loss_d: 1.400, loss_g: 0.761\n",
      "[40/5000]: loss_d: 1.399, loss_g: 0.815\n",
      "[41/5000]: loss_d: 1.392, loss_g: 0.776\n",
      "[42/5000]: loss_d: 1.380, loss_g: 0.784\n",
      "[43/5000]: loss_d: 1.382, loss_g: 0.798\n",
      "[44/5000]: loss_d: 1.406, loss_g: 0.790\n",
      "[45/5000]: loss_d: 1.391, loss_g: 0.776\n",
      "[46/5000]: loss_d: 1.385, loss_g: 0.793\n",
      "[47/5000]: loss_d: 1.388, loss_g: 0.788\n",
      "[48/5000]: loss_d: 1.397, loss_g: 0.796\n",
      "[49/5000]: loss_d: 1.391, loss_g: 0.823\n",
      "[50/5000]: loss_d: 1.383, loss_g: 0.783\n",
      "[51/5000]: loss_d: 1.395, loss_g: 0.796\n",
      "[52/5000]: loss_d: 1.387, loss_g: 0.794\n",
      "[53/5000]: loss_d: 1.383, loss_g: 0.777\n",
      "[54/5000]: loss_d: 1.401, loss_g: 0.825\n",
      "[55/5000]: loss_d: 1.377, loss_g: 0.782\n",
      "[56/5000]: loss_d: 1.402, loss_g: 0.773\n",
      "[57/5000]: loss_d: 1.395, loss_g: 0.798\n",
      "[58/5000]: loss_d: 1.399, loss_g: 0.792\n",
      "[59/5000]: loss_d: 1.368, loss_g: 0.799\n",
      "[60/5000]: loss_d: 1.402, loss_g: 0.796\n",
      "[61/5000]: loss_d: 1.384, loss_g: 0.792\n",
      "[62/5000]: loss_d: 1.402, loss_g: 0.829\n",
      "[63/5000]: loss_d: 1.371, loss_g: 0.782\n",
      "[64/5000]: loss_d: 1.381, loss_g: 0.797\n",
      "[65/5000]: loss_d: 1.399, loss_g: 0.808\n",
      "[66/5000]: loss_d: 1.398, loss_g: 0.788\n",
      "[67/5000]: loss_d: 1.381, loss_g: 0.802\n",
      "[68/5000]: loss_d: 1.395, loss_g: 0.797\n",
      "[69/5000]: loss_d: 1.385, loss_g: 0.795\n",
      "[70/5000]: loss_d: 1.388, loss_g: 0.826\n",
      "[71/5000]: loss_d: 1.388, loss_g: 0.808\n",
      "[72/5000]: loss_d: 1.404, loss_g: 0.802\n",
      "[73/5000]: loss_d: 1.388, loss_g: 0.792\n",
      "[74/5000]: loss_d: 1.382, loss_g: 0.803\n",
      "[75/5000]: loss_d: 1.396, loss_g: 0.807\n",
      "[76/5000]: loss_d: 1.405, loss_g: 0.785\n",
      "[77/5000]: loss_d: 1.390, loss_g: 0.817\n",
      "[78/5000]: loss_d: 1.369, loss_g: 0.808\n",
      "[79/5000]: loss_d: 1.392, loss_g: 0.814\n",
      "[80/5000]: loss_d: 1.393, loss_g: 0.796\n",
      "[81/5000]: loss_d: 1.396, loss_g: 0.794\n",
      "[82/5000]: loss_d: 1.374, loss_g: 0.779\n",
      "[83/5000]: loss_d: 1.390, loss_g: 0.816\n",
      "[84/5000]: loss_d: 1.410, loss_g: 0.816\n",
      "[85/5000]: loss_d: 1.391, loss_g: 0.799\n",
      "[86/5000]: loss_d: 1.405, loss_g: 0.820\n",
      "[87/5000]: loss_d: 1.365, loss_g: 0.798\n",
      "[88/5000]: loss_d: 1.389, loss_g: 0.818\n",
      "[89/5000]: loss_d: 1.385, loss_g: 0.787\n",
      "[90/5000]: loss_d: 1.396, loss_g: 0.826\n",
      "[91/5000]: loss_d: 1.375, loss_g: 0.793\n",
      "[92/5000]: loss_d: 1.392, loss_g: 0.802\n",
      "[93/5000]: loss_d: 1.376, loss_g: 0.796\n",
      "[94/5000]: loss_d: 1.405, loss_g: 0.800\n",
      "[95/5000]: loss_d: 1.388, loss_g: 0.802\n",
      "[96/5000]: loss_d: 1.394, loss_g: 0.831\n",
      "[97/5000]: loss_d: 1.378, loss_g: 0.787\n",
      "[98/5000]: loss_d: 1.389, loss_g: 0.798\n",
      "[99/5000]: loss_d: 1.379, loss_g: 0.809\n",
      "[100/5000]: loss_d: 1.386, loss_g: 0.812\n",
      "[101/5000]: loss_d: 1.379, loss_g: 0.812\n",
      "[102/5000]: loss_d: 1.378, loss_g: 0.820\n",
      "[103/5000]: loss_d: 1.391, loss_g: 0.805\n",
      "[104/5000]: loss_d: 1.385, loss_g: 0.806\n",
      "[105/5000]: loss_d: 1.389, loss_g: 0.802\n",
      "[106/5000]: loss_d: 1.397, loss_g: 0.876\n",
      "[107/5000]: loss_d: 1.389, loss_g: 0.831\n",
      "[108/5000]: loss_d: 1.370, loss_g: 0.810\n",
      "[109/5000]: loss_d: 1.405, loss_g: 0.819\n",
      "[110/5000]: loss_d: 1.397, loss_g: 0.813\n",
      "[111/5000]: loss_d: 1.369, loss_g: 0.830\n",
      "[112/5000]: loss_d: 1.398, loss_g: 0.820\n",
      "[113/5000]: loss_d: 1.398, loss_g: 0.814\n",
      "[114/5000]: loss_d: 1.371, loss_g: 0.819\n",
      "[115/5000]: loss_d: 1.373, loss_g: 0.818\n",
      "[116/5000]: loss_d: 1.387, loss_g: 0.809\n",
      "[117/5000]: loss_d: 1.398, loss_g: 0.833\n",
      "[118/5000]: loss_d: 1.403, loss_g: 0.810\n",
      "[119/5000]: loss_d: 1.374, loss_g: 0.813\n",
      "[120/5000]: loss_d: 1.371, loss_g: 0.839\n",
      "[121/5000]: loss_d: 1.379, loss_g: 0.816\n",
      "[122/5000]: loss_d: 1.398, loss_g: 0.831\n",
      "[123/5000]: loss_d: 1.376, loss_g: 0.799\n",
      "[124/5000]: loss_d: 1.370, loss_g: 0.833\n",
      "[125/5000]: loss_d: 1.371, loss_g: 0.821\n",
      "[126/5000]: loss_d: 1.383, loss_g: 0.835\n",
      "[127/5000]: loss_d: 1.378, loss_g: 0.834\n",
      "[128/5000]: loss_d: 1.372, loss_g: 0.822\n",
      "[129/5000]: loss_d: 1.387, loss_g: 0.841\n",
      "[130/5000]: loss_d: 1.372, loss_g: 0.815\n",
      "[131/5000]: loss_d: 1.372, loss_g: 0.833\n",
      "[132/5000]: loss_d: 1.374, loss_g: 0.825\n",
      "[133/5000]: loss_d: 1.391, loss_g: 0.854\n",
      "[134/5000]: loss_d: 1.376, loss_g: 0.826\n",
      "[135/5000]: loss_d: 1.378, loss_g: 0.821\n",
      "[136/5000]: loss_d: 1.380, loss_g: 0.835\n",
      "[137/5000]: loss_d: 1.384, loss_g: 0.835\n",
      "[138/5000]: loss_d: 1.365, loss_g: 0.809\n",
      "[139/5000]: loss_d: 1.378, loss_g: 0.839\n",
      "[140/5000]: loss_d: 1.374, loss_g: 0.838\n",
      "[141/5000]: loss_d: 1.375, loss_g: 0.837\n",
      "[142/5000]: loss_d: 1.402, loss_g: 0.834\n",
      "[143/5000]: loss_d: 1.382, loss_g: 0.829\n",
      "[144/5000]: loss_d: 1.390, loss_g: 0.837\n",
      "[145/5000]: loss_d: 1.387, loss_g: 0.838\n",
      "[146/5000]: loss_d: 1.385, loss_g: 0.840\n",
      "[147/5000]: loss_d: 1.386, loss_g: 0.839\n",
      "[148/5000]: loss_d: 1.388, loss_g: 0.852\n",
      "[149/5000]: loss_d: 1.381, loss_g: 0.826\n",
      "[150/5000]: loss_d: 1.402, loss_g: 0.838\n",
      "[151/5000]: loss_d: 1.379, loss_g: 0.839\n",
      "[152/5000]: loss_d: 1.404, loss_g: 0.834\n",
      "[153/5000]: loss_d: 1.399, loss_g: 0.843\n",
      "[154/5000]: loss_d: 1.385, loss_g: 0.808\n",
      "[155/5000]: loss_d: 1.399, loss_g: 0.836\n",
      "[156/5000]: loss_d: 1.385, loss_g: 0.824\n",
      "[157/5000]: loss_d: 1.408, loss_g: 0.824\n",
      "[158/5000]: loss_d: 1.394, loss_g: 0.818\n",
      "[159/5000]: loss_d: 1.401, loss_g: 0.834\n",
      "[160/5000]: loss_d: 1.399, loss_g: 0.806\n",
      "[161/5000]: loss_d: 1.395, loss_g: 0.828\n",
      "[162/5000]: loss_d: 1.389, loss_g: 0.808\n",
      "[163/5000]: loss_d: 1.394, loss_g: 0.818\n",
      "[164/5000]: loss_d: 1.436, loss_g: 0.854\n",
      "[165/5000]: loss_d: 1.366, loss_g: 0.813\n",
      "[166/5000]: loss_d: 1.408, loss_g: 1.024\n",
      "[167/5000]: loss_d: 1.390, loss_g: 0.783\n",
      "[168/5000]: loss_d: 1.365, loss_g: 0.812\n",
      "[169/5000]: loss_d: 1.368, loss_g: 0.837\n",
      "[170/5000]: loss_d: 1.395, loss_g: 0.854\n",
      "[171/5000]: loss_d: 1.385, loss_g: 0.826\n",
      "[172/5000]: loss_d: 1.387, loss_g: 0.835\n",
      "[173/5000]: loss_d: 1.385, loss_g: 0.836\n",
      "[174/5000]: loss_d: 1.407, loss_g: 0.809\n",
      "[175/5000]: loss_d: 1.400, loss_g: 0.801\n",
      "[176/5000]: loss_d: 1.426, loss_g: 0.857\n",
      "[177/5000]: loss_d: 1.377, loss_g: 0.831\n",
      "[178/5000]: loss_d: 1.413, loss_g: 0.823\n",
      "[179/5000]: loss_d: 1.399, loss_g: 0.842\n",
      "[180/5000]: loss_d: 1.411, loss_g: 0.824\n",
      "[181/5000]: loss_d: 1.392, loss_g: 0.854\n"
     ]
    }
   ],
   "source": [
    "des_path = project_root + 'results/'\n",
    "if not os.path.exists(des_path):\n",
    "    os.makedirs(des_path, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    train(epoch)\n",
    "#     test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(BATCH_SIZE, g_input_size).to(device)\n",
    "        sample = G(sample).cpu()\n",
    "        save_image(sample.view(BATCH_SIZE, 1, 28, 28),\n",
    "                   f'{des_path}epoch_{epoch}.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练极不稳定，随着训练过程发展会出现：\n",
    "- 判别器损失趋近于0， 生成器损失增大，说明学得了很好的判别器，而生成器就被压制了\n",
    "\n",
    "现在训练数据位于[0,1)之间，规范化到[-1,1]之间效果会怎样？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
