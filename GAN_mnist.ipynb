{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.realpath('.')\n",
    "print(project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "no_cuda = False\n",
    "cuda_available = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 100\n",
    "SEED = 8\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda_available else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./MNIST_data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./MNIST_data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "g_input_size = 100     # Random noise dimension coming into generator, per output vector\n",
    "\n",
    "g_output_size = 784    # size of generated output vector\n",
    "\n",
    "d_input_size = 784   # Minibatch size - cardinality of distributions\n",
    "\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
    "\n",
    "\n",
    "d_learning_rate = 2e-4  # 2e-4\n",
    "g_learning_rate = 2e-4\n",
    "optim_betas = (0.9, 0.999)\n",
    "\n",
    "print_interval = 200\n",
    "\n",
    "d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "g_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(g_input_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(1024, g_output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), g_input_size)\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(d_input_size, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        out = self.model(x.view(x.size(0), d_input_size))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D  = Discriminator().to(device)\n",
    "G = Generator().to(device)\n",
    "print(D)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "lr = 0.0002\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_losses = []\n",
    "G_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    D.train()\n",
    "    G.train()\n",
    "    \n",
    "    D_losses.clear()\n",
    "    G_losses.clear()\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        real_data = data.to(device) \n",
    "        D.zero_grad()\n",
    "        \n",
    "        for d_index in range(d_steps):\n",
    "            # 1. Train D on real+fake\n",
    "            #  1A: Train D on real\n",
    "            d_real_decision = D(Variable(real_data))\n",
    "#             if d_real_decision.shape[0] != BATCH_SIZE:\n",
    "#                 print(d_real_decision.shape)\n",
    "            d_real_error = criterion(d_real_decision, Variable(torch.ones(data.shape[0],1)).to(device))  # ones = true\n",
    "            d_real_error.backward() # compute/store gradients, but don't change params\n",
    "            \n",
    "            #  1B: Train D on fake\n",
    "            d_gen_input = Variable(torch.randn(data.shape[0], g_input_size))\n",
    "            d_fake_data = G(d_gen_input.to(device)).detach()  # detach to avoid training G on these labels\n",
    "            \n",
    "            d_fake_decision = D(d_fake_data.to(device))\n",
    "            d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(data.shape[0], 1)).to(device))  # zeros = fake\n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "            \n",
    "            d_error = d_real_error + d_fake_error\n",
    "            D_losses.append(d_error.data)\n",
    "\n",
    "        for g_index in range(g_steps):\n",
    "        # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.zero_grad()\n",
    "\n",
    "            gen_input = Variable(torch.randn(data.shape[0], g_input_size).to(device))\n",
    "            g_fake_data = G(gen_input)\n",
    "            dg_fake_decision = D(g_fake_data.to(device))\n",
    "            g_error = criterion(dg_fake_decision, Variable(torch.ones(data.shape[0], 1)).to(device))  # we want to fool, so pretend it's all genuine\n",
    "\n",
    "            g_error.backward()\n",
    "            g_optimizer.step()  # Only optimizes G's parameters\n",
    "            \n",
    "            G_losses.append(g_error.data)\n",
    "        \n",
    "            \n",
    "    print('[%d/%d]: D(x): %.3f, D(G(z)): %.3f' % (\n",
    "        epoch , EPOCH,np.mean(D_losses), np.mean(G_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    G.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_path = os.path.join(project_root, 'results/')\n",
    "if not os.path.exists(des_path):\n",
    "    os.makedirs(des_path, exist_ok=True)\n",
    "\n",
    "import math,  itertools\n",
    "from IPython import display\n",
    "\n",
    "size_figure_grid = int(math.sqrt(BATCH_SIZE))\n",
    "fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(6, 6))\n",
    "for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "    ax[i,j].get_xaxis().set_visible(False)\n",
    "    ax[i,j].get_yaxis().set_visible(False)\n",
    "    \n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    display.clear_output(wait=True)\n",
    "    train(epoch)\n",
    "#     test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(BATCH_SIZE, g_input_size).to(device)\n",
    "        sample = G(sample).cpu()\n",
    "        save_image(sample.view(BATCH_SIZE, 1, 28, 28),\n",
    "                   f'{des_path}epoch_{epoch}.png')\n",
    "        \n",
    "        for k in range(BATCH_SIZE):\n",
    "            i = k//8\n",
    "            j = k%8\n",
    "            ax[i,j].cla()\n",
    "            ax[i,j].imshow(sample[k,:].data.cpu().numpy().reshape(28, 28),cmap='Greys')\n",
    "        \n",
    "        display.display(plt.gcf())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
